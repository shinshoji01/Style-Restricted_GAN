{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch \n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../pyfiles/\")\n",
    "from util import image_from_output, weights_init, get_target, cuda2numpy\n",
    "from dataset import get_class_label, FaceDataset\n",
    "from model import MinMax, SingleGenerator, SingleDiscriminator_solo_multi, Encoder_original\n",
    "from util_notebook import SingleGAN_training, get_output_and_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "In this notebook, we're going to explain the code to experiment with SingleGAN with a solo Discriminator proposed in StarGAN, focusing on the training procedure. The results will be examined in `02-test_SingleGAN_soloD.ipynb`. The main purpose of this notebook is to check if a solo disciminator still works in proposed losses.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CelebA Dataset\n",
    "---\n",
    "In this experiment, I'm gonna use the CelebA dataset, a face dataset that has plenty of annotations. We're therefore able to set various classes or conditions. I explained the details in `A_CelebA_dataset_usage.ipynb`.\n",
    "\n",
    "---\n",
    "### Preparation\n",
    "---\n",
    "First of all, navigate `root` to the root directory of the dataset and `label_root` to the directory of the label folder. `A_CelebA_dataset_usage.ipynb` also includes how the make the label folder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./../../dataset/celebA/img/\"\n",
    "label_root = \"./../../dataset/celebA/label_folder/\"\n",
    "# root = \"./../../research/sound_dataset/celebA/img_align_celeba_png/\"\n",
    "# label_root = \"./../../research/sound_dataset/celebA/label_folder/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As I said, the CelebA dataset has plenty of annotations, so we need to define which annotation we're gonna use.\n",
    "There are 3 ways to categorize the usage of the annotations.\n",
    "\n",
    "- ***existed***: Get the images that belong to the selected annotations.\n",
    "- ***delete***: Delete the images which belong to the selected annotations.\n",
    "- ***class***: These annotations are used to compose the classes, \\# of classes will be $2^{\\#\\_classes}$\n",
    "\n",
    "In this experiment, I used some annotations owing to simplicity and \n",
    "set the classes by the use of annotations; \"male\", \"smiling\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 5 o clock shadow\n",
    "# 11 blurry\n",
    "# 14 chubby\n",
    "# 15 double chin\n",
    "# 16 eyeglasses\n",
    "# 17 goatee\n",
    "# 21 male\n",
    "# 23 mustache\n",
    "# 25 No_Beard\n",
    "# 31 sideburns\n",
    "# 32 smiling\n",
    "# 36 wearing hat\n",
    "dataset_label = {}\n",
    "dataset_label[\"existed\"] = [25] \n",
    "dataset_label[\"delete\"] = [1, 11, 14, 15, 16, 17, 23, 31, 36] \n",
    "dataset_label[\"class\"] = [21, 32] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_discription = [\n",
    "    \"male, smiling\",\n",
    "    \"male, not smiling\",\n",
    "    \"female, smiling\",\n",
    "    \"female, not smiling\"\n",
    "]\n",
    "\n",
    "classes = tuple(range(2**len(dataset_label[\"class\"])))\n",
    "cl = get_class_label(len(dataset_label[\"class\"]))\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Preprocessing\n",
    "---\n",
    "Firstly, The image in the dataset is center-cropped to be the shape (178, 178) and then resized to be the shape (128, 128). In the training process, the random horizontal flip is installed to augment the dataset. When it comes to the value of the image, it's normalized to a range of [-1, 1].\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {}\n",
    "transform[\"train\"] = transforms.Compose([\n",
    "    transforms.CenterCrop((178, 178)),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    MinMax(True)\n",
    "])\n",
    "    \n",
    "transform[\"test\"] = transforms.Compose([\n",
    "    transforms.CenterCrop((178, 178)),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    MinMax(True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's have a look at some samples of the dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-39851b201b50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "width = 5\n",
    "length = 4\n",
    "dataset = FaceDataset(root, label_root, transform[\"train\"], dataset_label, classes, data_type=\"train\", train_num=10000, val_num=0, test_num=100)\n",
    "fig = plt.figure(figsize=(4*width, 4.5*length))\n",
    "for i in range(width*length):\n",
    "    ax = fig.add_subplot(length, width, i+1)\n",
    "    index = np.random.randint(0,len(dataset))\n",
    "    data = dataset[index]\n",
    "    img = data[0]\n",
    "    image = image_from_output(torch.reshape(img, (1,img.shape[0],img.shape[1],img.shape[2])))[0]\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(label_discription[data[1]], fontsize=20)\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "---\n",
    "Let's train the models.\n",
    "\n",
    "---\n",
    "### Preparation\n",
    "---\n",
    "At first, let's define some parameters for the architecture of the models, including a generator, a discriminator, and an encoder.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 8 # dimension of the latent code for style\n",
    "nch_in = 3 # the number of channels for input images \n",
    "\n",
    "# Hyper-parameters for a generator\n",
    "nch = 64 # the number of channels for the first convolutional layer\n",
    "reduce = 2 # image's shape is reduced by a factor of this value\n",
    "num_cls = 2 # the number of convolutional blocks for compression and expansion\n",
    "res_num = 6 # the number of residual blocks\n",
    "\n",
    "# Hyper-parameters for a generator\n",
    "dis_reduce = 2 # image's shape is reduced by a factor of this value\n",
    "dis_nch = 64 # the number of channels for the first convolutional layer\n",
    "dis_num_cls = 4 # the number of convolutional blocks for compression\n",
    "save_parameter = True # save parameters for the models or not\n",
    "criterion = nn.MSELoss() # evaluation metrics for discriminator's output\n",
    "criterion_class = nn.MSELoss() # evaluation metrics for discriminator's output (classification)\n",
    "ref_label = np.eye(len(classes)) # class label: one-hot label is employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, define more detailed information for the models and the lambda parameters for training.\n",
    "In this experiment, we're just going to check the possibility of the employment of a solo discriminator so there is no parameter to adjust.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 31 # the number of epochs\n",
    "lr_D = 0.0001 # initial learning rate for a Discriminator\n",
    "lr_G = 0.0002 # initial learning rate for a Generator\n",
    "lr_E = 0.001  # initial learning rate for an Encoder\n",
    "\n",
    "restriction_type = \"proposedKL\" # \"conventionalKL\" or \"proposedKL\": employed losses for restriction of the encoder's distribution\n",
    "if restriction_type == \"conventionalKL\":\n",
    "    encoded_feature = \"latent\" # \"latent\" or \"mu\": the encoded features used for regression loss\n",
    "elif restriction_type == \"proposedKL\":\n",
    "    encoded_feature = \"mu\" # \"latent\" or \"mu\": the encoded features used for regression loss\n",
    "\n",
    "lbd = {}\n",
    "lbd[\"class\"] = 1 # loss for auxiliary classifier\n",
    "lbd[\"cycle\"] = 5 # cycle consistency loss\n",
    "lbd[\"idt\"] = 5 # identity loss\n",
    "lbd[\"reg\"] = 0.5 # conventional regression loss\n",
    "lbd[\"idt_reg\"] = 0.5 # regression loss for indentity images\n",
    "    \n",
    "if restriction_type == \"conventionalKL\":\n",
    "    lbd[\"KL\"] = 0.1 # conventional KL divergence loss\n",
    "    lbd[\"batch_KL\"] = 0 # batch KL dvergence loss\n",
    "    lbd[\"corr_enc\"] = 0 # correlation loss\n",
    "    lbd[\"hist\"] = 0 # histogram imitation loss\n",
    "elif restriction_type == \"proposedKL\":\n",
    "    lbd[\"KL\"] = 0 # conventional KL divergence loss\n",
    "    lbd[\"batch_KL\"] = 10 # batch KL dvergence loss\n",
    "    lbd[\"corr_enc\"] = 100 # correlation loss\n",
    "    lbd[\"hist\"] = 100 # histogram imitation loss\n",
    "    \n",
    "unrolled_k = 5 # k for UnrolledGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "call training and sample dataset. The latter one is used to observe the result while training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f36c85b0ca10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m###############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msampleset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "############ adjustable parameters ############\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "###############################################\n",
    "###############################################\n",
    "\n",
    "dataset = FaceDataset(root, label_root, transform[\"train\"], dataset_label, classes, data_type=\"train\", train_num=10000, val_num=0, test_num=100)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "sampleset = FaceDataset(root, label_root, transform[\"test\"], dataset_label, classes, data_type=\"test\", train_num=10000, val_num=0, test_num=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Then, let's call the selected models. In our paper, we exploit multiple GPUs to reduce the training time. You can change the list of GPU you're gonna use.\n",
    "\n",
    "- `devices`: the list of the GPU's id you will use\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############ adjustable parameters ############\n",
    "\n",
    "devices = [0,1,2,3]\n",
    "\n",
    "###############################################\n",
    "###############################################\n",
    "\n",
    "\n",
    "netG = SingleGenerator(nch_in, nch, reduce, num_cls, res_num, \"instance\", num_con=ref_label.shape[1]+ndim).to(device)\n",
    "netG.apply(weights_init)\n",
    "netG = nn.DataParallel(netG, devices)\n",
    "netD = SingleDiscriminator_solo_multi(nch_in, dis_nch, dis_reduce, dis_num_cls, \"instance\", ref_label.shape[1]).to(device)\n",
    "netD.apply(weights_init)\n",
    "netD = nn.DataParallel(netD, devices)\n",
    "netE = Encoder_original(nch_in, ndim, nch, 4, \"instance\", ref_label.shape[1]).to(device)\n",
    "netE.apply(weights_init)\n",
    "netE = nn.DataParallel(netE, devices)\n",
    "sg = SingleGAN_training([netG, netD, netE], [None, None, None], [criterion, criterion_class], \n",
    "                       lbd, unrolled_k, device, ref_label, ndim, classes, batch_size, encoded_feature, True)\n",
    "sg.opt_sche_initialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's train your model. Since it takes time to process even just a single epoch, it shows the result 3 times in every epoch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train = True\n",
    "if run_train:\n",
    "    interval = int(len(dataset)/batch_size/3) + 2\n",
    "    losses_D = []\n",
    "    losses_G = []\n",
    "    losses_E = []\n",
    "    for epoch in range(epoch_num):\n",
    "        start_time = time.time()\n",
    "        loss_D = []\n",
    "        loss_G = []\n",
    "        loss_E = []\n",
    "        for itr, data in enumerate(dataloader):\n",
    "            netG.train()\n",
    "            data_image = data[0]\n",
    "            data_label = data[1]\n",
    "\n",
    "            source_image = data_image.to(device)\n",
    "            label = {}\n",
    "\n",
    "            label[\"source\"] = data_label.to(device)\n",
    "            whole_target = get_target(data_label, classes, whole=False)\n",
    "            label[\"target\"] = torch.tensor(whole_target[:,0], dtype=torch.long, device=\"cpu\")\n",
    "\n",
    "            errG, errD, errE = sg.train(source_image, label)\n",
    "\n",
    "            if type(errD)!=int:\n",
    "                errD = errD.detach().to(\"cpu\").numpy()\n",
    "                loss_D.append(errD)\n",
    "            if type(errG)!=int:\n",
    "                errG = errG.detach().to(\"cpu\").numpy()\n",
    "                loss_G.append(errG)\n",
    "            if type(errE)!=int:\n",
    "                errE = errE.detach().to(\"cpu\").numpy()\n",
    "                loss_E.append(errE)\n",
    "\n",
    "            if itr%interval==0:\n",
    "                if not itr==0:\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"epoch {epoch} :itr {itr}/{int(len(dataset)/batch_size)}: {(time.time()-start_time)/60} mins\")\n",
    "                    fig = get_output_and_plot(sg, sampleset, 53, [classes, label_discription], 5, False, device)\n",
    "\n",
    "                    if not epoch==0:\n",
    "                        length = 5\n",
    "                        width = 4\n",
    "                        losses = [losses_D, losses_G, losses_E]\n",
    "                        ax = fig.add_subplot(length, width, 17)\n",
    "                        models = [\"Discriminator\", \"Generator\", \"Encoder\"]\n",
    "                        for k in range(len(losses)):\n",
    "                            if models[k] == \"Discriminator\":\n",
    "                                if type(losses[k]) == dict:\n",
    "                                    for i in classes:\n",
    "                                        ax.plot(losses[k][i], label=f\"Discriminator-{i}\")\n",
    "                                elif type(losses[k]) == list:\n",
    "                                    ax.plot(losses[k], label=f\"Discriminator\")\n",
    "                            else:\n",
    "                                ax.plot(losses[k], label=models[k])\n",
    "                        ax.legend()\n",
    "                    plt.show()\n",
    "\n",
    "        if save_parameter:\n",
    "            if epoch % 3 == 0:\n",
    "                dir = \"./instant_model_parameter/\"\n",
    "                gen_path = dir + f\"gen_singleD_SingleGAN_idt{lbd['idt']}_cycle{lbd['cycle']}_KL{lbd['KL']}_reg{lbd['reg']}_idtreg{lbd['idt_reg']}_bKL{lbd['batch_KL']}_correnc{lbd['corr_enc']}_hist{lbd['hist']}_unrolledk{unrolled_k}_epoch{epoch}\"\n",
    "                dis_path = dir + f\"dis_singleD_SingleGAN_idt{lbd['idt']}_cycle{lbd['cycle']}_KL{lbd['KL']}_reg{lbd['reg']}_idtreg{lbd['idt_reg']}_bKL{lbd['batch_KL']}_correnc{lbd['corr_enc']}_hist{lbd['hist']}_unrolledk{unrolled_k}_epoch{epoch}\"\n",
    "                enc_path = dir + f\"enc_singleD_SingleGAN_idt{lbd['idt']}_cycle{lbd['cycle']}_KL{lbd['KL']}_reg{lbd['reg']}_idtreg{lbd['idt_reg']}_bKL{lbd['batch_KL']}_correnc{lbd['corr_enc']}_hist{lbd['hist']}_unrolledk{unrolled_k}_epoch{epoch}\"\n",
    "\n",
    "                torch.save(sg.G.module.state_dict(), gen_path)\n",
    "                torch.save(sg.D.module.state_dict(), dis_path)\n",
    "                torch.save(sg.E.module.state_dict(), enc_path)\n",
    "\n",
    "        sg.scheD.step()\n",
    "        losses_D.append(np.mean(loss_D))\n",
    "        sg.scheG.step()\n",
    "        losses_G.append(np.mean(loss_G))\n",
    "        sg.scheE.step()\n",
    "        losses_E.append(np.mean(loss_E))\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"{epoch+1}: {(time.time()-start_time)/60} mins\")\n",
    "        fig = get_output_and_plot(sg, sampleset, 53, [classes, label_discription], 5, False, device)\n",
    "\n",
    "        length = 5\n",
    "        width = 4\n",
    "        losses = [losses_D, losses_G, losses_E]\n",
    "        ax = fig.add_subplot(length, width, 17)\n",
    "        models = [\"Discriminator\", \"Generator\", \"Encoder\"]\n",
    "        for k in range(len(losses)):\n",
    "            if models[k] == \"Discriminator\":\n",
    "                if type(losses[k]) == dict:\n",
    "                    for i in classes:\n",
    "                        ax.plot(losses[k][i], label=f\"Discriminator-{i}\")\n",
    "                elif type(losses[k]) == list:\n",
    "                    ax.plot(losses[k], label=f\"Discriminator\")\n",
    "            else:\n",
    "                ax.plot(losses[k], label=models[k])\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
